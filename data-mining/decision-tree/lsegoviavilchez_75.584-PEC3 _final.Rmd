---
title: 'Minería de datos: PEC3 - Clasificación con árboles de decisión'
author: "Autor: Leonardo Segovia Vilchez"
date: "Mayo 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)
```


******
# Introducción
******
## Presentación
Esta prueba de evaluación continua cubre los Módulos 3 (Clasificación:
árboles de decisión) y el Módulo 8 (Evaluación de modelos) del programa de la asignatura.

## Competencias
Las competencias que se trabajan en esta prueba son:  

* Uso y aplicación de las TIC en el ámbito académico y profesional.
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.

## Objetivos
La correcta asimilación del Módulo 3. En esta PEC trabajaremos la generación e interpretación de un árbol de decisión con el software de prácticas. Seguiremos también con la preparación de los datos y la extracción inicial de conocimiento.

## Descripción de la PEC a realizar
La prueba está estructurada en un total de un único ejercicio práctico.

## Recursos Básicos
**Material docente proporcionado por la UOC.** 

Módulo 3 y 8 del material didáctico.

**Complementarios** 

* Los descritos para la anterior PEC.
* Fichero titanic.csv
* R package C5.0 (decisión Trees and Rule-Based Models): https://cran.r-project.org/web/packages/C50/index.html


## Criterios de valoración

Todos los ejercicios deben ser presentados de forma razonada y clara, especificando todos y cada uno de los pasos que se hayan llevado a cabo para su resolución. No se aceptará ninguna respuesta que no esté claramente justificada.

## Formato y fecha de entega
El formato de entrega es: usernameestudiant-PECn.html/doc/docx/odt/pdf.
Se recomienda la entrega en formato html y también el Rmd que genera el html entregado.
Fecha de Entrega: 20/05/2020.
Se debe entregar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en qué se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia  no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Enunciado  
******

En este ejercicio vamos a seguir los pasos del ciclo de vida de un proyecto de minería de datos, para el caso de un algoritmo de clasificación y más concretamente un árbol de decisión. Lo haremos con el archivo titanic.csv, que se encuentra adjunto en el aula. Este archivo contiene un registro por cada pasajero que viajaba en el Titanic. En las variables se caracteriza si era hombre o mujer, adulto o menor (niño), en qué categoría viajaba o si era miembro de la tripulación.

Objetivos:

*	Estudiar los datos, por ejemplo: ¿Número de registros del fichero? ¿Distribuciones de valores por variables? ¿Hay campos mal informados o vacíos?
*	Preparar los datos. En este caso ya están en el formato correcto y no es necesario discretizar ni generar atributos nuevos. Hay que elegir cuáles son las variables que se utilizarán para construir el modelo y cuál es la variable que clasifica. En este caso la variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no.
*	Instalar, si es necesario, el paquete C5.0  Se trata de una implementación más moderna del algoritmo ID3 de Quinlan. Tiene los principios teóricos del ID3 más la poda automática. Con este paquete generar un modelo de minería.
*	¿Cuál es la calidad del modelo?
*	Generar el árbol gráfico.
* Generar y extraer las reglas del modelo.
*	En función del modelo, el árbol y las reglas: ¿Cuál es el conocimiento que obtenemos?
*	Probar el modelo generado presentándole nuevos registros. ¿Clasifica suficientemente bien?
  
##  Revisión de los datos, extracción visual de información y preparación de los datos

Carga de los datos:

```{r message= FALSE, warning=FALSE}
data<-read.csv("./titanic.csv",header=T,sep=",")
attach(data)
```


Empezaremos haciendo un breve análisis de los datos ya que nos interesa tener una idea general de los datos que disponemos. Por ello, primero calcularemos las dimensiones de nuestra base de datos y analizaremos qué tipos de atributos tenemos.

Para empezar, calculamos las dimensiones de la base de datos mediante la función dim(). Obtenemos que disponemos de 2201 registros o pasajeros (filas) y 4 variables (columnas). 

```{r}
dim(data)
```

¿Cuáles son esas variables? Gracias a la función str() sabemos que las cuatro variables son categóricas o discretas, es decir, toman valores en un conjunto finito. La variable CLASS hace referencia a la clase en la que viajaban los pasajeros (1ª, 2ª, 3ª o crew), AGE determina si era adulto o niño (Adulto o Menor), la variable SEX si era hombre o mujer (Hombre o Mujer) y la última variable (SURVIVED) informa si el pasajero murió o sobrevivió en el accidente (Muere o Sobrevive).

```{r}
str(data)
```

Es de gran interés saber si tenemos muchos valores nulos (campos vacíos) y la distribución de valores por variables. Es por ello recomendable empezar el análisis con una visión general de las variables. Mostraremos para cada atributo la cantidad de valores perdidos mediante la función summary.  

```{r}
summary(data)
```

Disponemos por tanto de un data frame formado por cuatro variables categóricas sin valores nulos. Para un conocimiento mayor sobre los datos, tenemos a nuestro alcance unas herramientas muy valiosas: las herramientas de visualización. Para dichas visualizaciones, haremos uso de los paquetes ggplot2, gridExtra y grid de R. 

```{r}
if(!require(ggplot2)){
    install.packages('ggplot2', repos='http://cran.us.r-project.org')
    library(ggplot2)
}
if(!require(grid)){
    install.packages('grid', repos='http://cran.us.r-project.org')
    library(grid)
}
if(!require(gridExtra)){
    install.packages('gridExtra', repos='http://cran.us.r-project.org')
    library(gridExtra)
}

```


Nos interesa describir la relación entre la supervivencia y cada uno de las variables mencionadas anteriormente. Para ello, por un lado graficaremos mediante diagramas de barras la cantidad de muertos y supervivientes según la clase en la que viajaban, la edad o el sexo. Por otro lado, para obtener los datos que estamos graficando utilizaremos el comando table para dos variables que nos proporciona una tabla de contingencia.

```{r}
grid.newpage()
plotbyClass<-ggplot(data,aes(CLASS,fill=SURVIVED))+geom_bar() +labs(x="Class", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Class")
plotbyAge<-ggplot(data,aes(AGE,fill=SURVIVED))+geom_bar() +labs(x="Age", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Age")
plotbySex<-ggplot(data,aes(SEX,fill=SURVIVED))+geom_bar() +labs(x="Sex", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Sex")
grid.arrange(plotbyClass,plotbyAge,plotbySex,ncol=2)

```

De estos gráficos obtenemos información muy valiosa que complementamos con las tablas de contingencia (listadas abajo). Por un lado, la cantidad de pasajeros que sobrevivieron es similar en hombres y mujeres (hombres: 367 y mujeres 344). No, en cambio, si tenemos en cuenta el porcentaje respecto a su sexo. Es decir, pese a que la cantidad de mujeres y hombres que sobrevivieron es pareja, viajaban más hombres que mujeres (470 mujeres y 1731 hombres), por lo tanto, la tasa de muerte en hombres es muchísimo mayor (el 78,79% de los hombres murieron mientras que en mujeres ese porcentaje baja a 26,8%). 

En cuanto a la clase en la que viajaban, los pasajeros que viajaban en primera clase fueron los únicos que el porcentaje de supervivencia era mayor que el de mortalidad. El 62,46% de los viajeros de primera clase sobrevivió, el 41,4% de los que viajaban en segunda clase mientras que de los viajeros de tercera y de la tripulación solo sobrevivieron un 25,21% y 23,95% respectivamente. Para finalizar, destacamos que la presencia de pasajeros adultos era mucho mayor que la de los niños (2092 frente a 109) y que la tasa de supervivencia en niños fue mucho mayor (52,29% frente a 31,26%), no podemos obviar, en cambio, que los únicos niños que murieron fueron todos pasajeros de tercera clase (52 niños). 

```{r}
tabla_SST <- table(SEX, SURVIVED)
tabla_SST
prop.table(tabla_SST, margin = 1)
```

```{r}
tabla_SCT <- table(CLASS,SURVIVED)
tabla_SCT
prop.table(tabla_SCT, margin = 1)
```

```{r}
tabla_SAT <- table(AGE,SURVIVED)
tabla_SAT
prop.table(tabla_SAT, margin = 1) 
```

```{r}
tabla_SAT.byClass <- table(AGE,SURVIVED,CLASS)
tabla_SAT.byClass
```

Una alternativa interesante a las barras de diagramas, es el plot de las tablas de contingencia. Obtenemos la misma información pero para algunos receptores puede resultar más visual.  

```{r}
par(mfrow=c(2,2))
plot(tabla_SCT, col = c("black","#008000"), main = "SURVIVED vs. CLASS")
plot(tabla_SAT, col = c("black","#008000"), main = "SURVIVED vs. AGE")
plot(tabla_SST, col = c("black","#008000"), main = "SURVIVED vs. SEX")
```

Nuestro objetivo es crear un árbol de decisión que permita analizar qué tipo de pasajero del Titanic tenía probabilidades de sobrevivir o no. Por lo tanto, la variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no. De todas maneras, al imprimir las primeras (con head) y últimas 10 (con tail) filas nos damos cuenta de que los datos están ordenados.

```{r}
head(data,10)
tail(data,10)
```

Nos interesará "desordenarlos". Guardaremos los datos con el nuevo nombre como "data_random".

```{r}
set.seed(1)
data_random <- data[sample(nrow(data)),]
```

Para la futura evaluación del árbol de decisión, es necesario dividir el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba. El conjunto de entrenamiento es el subconjunto del conjunto original de datos utilizado para construir un primer modelo; y el conjunto de prueba, el subconjunto del conjunto original de datos utilizado para evaluar la calidad del modelo. 

Lo más correcto será utilizar un conjunto de datos diferente del que utilizamos para construir el árbol, es decir, un conjunto diferente del de entrenamiento. No hay ninguna proporción fijada con respecto al número relativo de componentes de cada subconjunto, pero la más utilizada acostumbra a ser 2/3 para el conjunto de entrenamiento y 1/3, para el conjunto de prueba. 

La variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no, que está en la cuarta columna.

```{r}
set.seed(666)
y <- data_random[,4] 
X <- data_random[,1:3] 
```


Podemos elegir el subconjunto de entrenamiento y de prueba de diversas maneras. La primer opción consiste en calcular a cuántas filas corresponde dos tercios de los datos (2*2201/3=1467) y dividir "manualmente" el conjunto.

```{r}
trainX <- X[1:1467,]
trainy <- y[1:1467]
testX <- X[1468:2201,]
testy <- y[1468:2201]
```

En la segunda opción podemos crear directamente un rango.

```{r}
indexes = sample(1:nrow(data), size=floor((2/3)*nrow(data)))
trainX<-X[indexes,]
trainy<-y[indexes]
testX<-X[-indexes,]
testy<-y[-indexes]
```

Después de una extracción aleatoria de casos es altamente recomendable efectuar un análisis de datos mínimo para asegurarnos de no obtener clasificadores sesgados por los valores que contiene cada muestra. 

## Creación del modelo, calidad del modelo y extracción de reglas

Se crea el árbol de decisión usando los datos de entrenamiento:

```{r}
  model <- C50::C5.0(trainX, trainy,rules=TRUE )
summary(model)
```

Errors muestra el número y porcentaje de casos mal clasificados en el subconjunto de entrenamiento. El árbol obtenido clasifica erróneamente 304 de los 1467 casos dados, una tasa de error del 20.7%.

A partir del árbol de decisión de dos hojas que hemos modelado, se pueden extraer las siguientes reglas de decisión (gracias a rules=TRUE podemos imprimir las reglas directamente):

SEX = "Hombre" → Muere. Validez: 80,2%

CLASS = "3a" → Muere. Validez: 75.1%

CLASS "1ª", "2ª" o "Crew" y SEX = "Mujer" → Sobrevive. Validez: 90,5%

Por tanto podemos concluir que el conocimiento extraído y cruzado con el análisis visual se resume en "las mujeres y los niños primero a excepción de que fueras de 3ª clase".

A continuación mostramos el árbol obtenido.

```{r}
model <- C50::C5.0(trainX, trainy)
plot(model)
```


## Validación del modelo con los datos reservados
Una vez tenemos el modelo, podemos comprobar su calidad prediciendo la clase para los datos de prueba que nos hemos reservado al principio. 

```{r}
predicted_model <- predict( model, testX, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))
```

Cuando hay pocas clases, la calidad de la predicción se puede analizar mediante una matriz de confusión que identifica los tipos de errores cometidos. 

```{r}
mat_conf<-table(testy,Predicted=predicted_model)
mat_conf
```

Otra manera de calcular el porcentaje de registros correctamente clasificados usando la matriz de confusión:

```{r}

porcentaje_correct<-100 * sum(diag(mat_conf)) / sum(mat_conf)
print(sprintf("El %% de registros correctamente clasificados es: %.4f %%",porcentaje_correct))

```

Además, tenemos a nuestra disposición el paquete gmodels para obtener información más completa:

```{r}
if(!require(gmodels)){
    install.packages('gmodels', repos='http://cran.us.r-project.org')
    library(gmodels)
}
```
```{r}
CrossTable(testy, predicted_model,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Reality', 'Prediction'))
```


******
# Ejercicios
******

## Ejercicio 1:  
Partiendo del ejemplo mostrado, repetid el ejercicio con otro conjunto de datos. Pueden ser datos reales de vuestro ámbito laboral o de algún repositorio de datos de Internet. Mirad por ejemplo: http://www.ics.uci.edu/~mlearn/MLSummary.html i http://www.kaggle.com.

Es muy importante seleccionar correctamente el conjunto de datos y explicar de forma correcta la base de datos y la razón de su elección.

Podéis añadir o variar los puntos si lo consideráis necesario (por ejemplo, crear el modelo con todos los datos y validación cruzada, probar el boosting o variar el prunning ...) Recordad también que el ciclo de vida de los proyectos de minería contempla retroceder para volver a generar el modelo con datos modificados o parámetros del algoritmo variados si el resultado no es lo suficientemente bueno.

### Respuesta 1:

Añadir aquí vuestro código:

```{r echo=TRUE, message=FALSE, warning=FALSE}

library(plyr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tibble)
library(dplyr)
library(ggplot2)
library(arules)
library(purrr)
library(dendextend)
library(scales)
library(tidyr)
```

El conjunto de datos contiene atributos cuantitativos y cualitativos siendo el último atributo la etiqueta corresponidente a la última hoje del árbol. El conjunto de datos es portanto apto para ser utilizarlo en un árbol de decisión ya que vamos a responder a una pregunta concreta, en este caso binaria, de sí o no. 

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Cargamos el juego de datos
rawDataCredit <- read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data',sep = " ", header = FALSE)

# Nombres de los atributos
colnames(rawDataCredit) <- c("statusAccount", "durationMonth", "creditHistory", "purpose", "creditAmount", "savingAccount", "presentEmployment", "rateIncome", "sex", "otherDebtors", "presentResidence", "property", "age", "otherInstall", "housing", "otherCredits", "job", "noPeople", "telephone", "foreign", "default")

```

Hacemos un primer análisis del dataset.

Nos interesa ver la dimensionalidad del dataset. Me gustaría seleccionar menos atributos ya que hay 21 atributos, esto puede generar un árbol difícil de interpretar visualmente. Así que más adelante haré una reducción de los atributos selecionando solo los relevantes basado en un anáisis previo. 
```{r}
dim(rawDataCredit)
```

Observamos que el dataset se compone de atributos categoricos y numéricos en este punto vemos que atributos debemos factorizar.
```{r}
str(rawDataCredit)
```
Observamos que debemos factorizar "default".

Me gustaría entrenar mi modelo para detectar que solicitud para obtener un crédito podría ser de riesgo basándome en el histórico disponile. La variable "default" indica cuando una soliditud de crédito no se pudo pagar por lo que haremos una clasificación binaria. 

En este caso no he selecionado todos los atributos que se muestran a continuación (solo los marcados con un *).

Descripcion del dataset:

*Attribute 1:  (qualitative) Status of existing checking account

*Attribute 2:  (numerical) Duration in month

*Attribute 3:  (qualitative) Credit history

*Attribute 4:  (qualitative) Purpose

Attribute 5:  (numerical) Credit amount

*Attribute 6:  (qualitative) Savings account/bonds

Attribute 7:  (qualitative) Present employment since

*Attribute 8:  (numerical) Installment rate in percentage of disposable income

Attribute 9:  (qualitative) Personal status and sex

Attribute 10: (qualitative) Other debtors / guarantors

Attribute 11: (numerical) Present residence since

Attribute 12: (qualitative) Property

*Attribute 13: (numerical) Age in years

Attribute 14: (qualitative) Other installment plans 

Attribute 15: (qualitative) Housing

*Attribute 16: (numerical) Number of existing credits at this bank

*Attribute 17: (qualitative) Job

Attribute 18: (numerical) Number of people being liable to provide maintenance for

Attribute 19: (qualitative) Telephone

Attribute 20: (qualitative) foreign worker

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Seleccionamos los atributos a trabajar.
dataCredit <- rawDataCredit %>%
  select(statusAccount, durationMonth, creditHistory, purpose, savingAccount, rateIncome, otherCredits, job, age, default)

# Mostramos los atributos.
glimpse(dataCredit)

```

Conprobamos la integridad de los datos. Comprobamos que no hay datos nulos o vacíos.
```{r message= FALSE, warning=FALSE}
# Confimamos que están todos los valores 
colSums(dataCredit == " " | dataCredit == "" | is.na(dataCredit))
```

Cambiamos el nombre de los factores para un mejor análisis.
```{r message= FALSE, warning=FALSE}

# Factorizamos el atributo "default" 1 -  impago,  0 = No hay impago
dataCredit$default <- dataCredit$default - 1
dataCredit$default <- as.factor(dataCredit$default)
dataCredit$otherCredits <- as.factor(dataCredit$otherCredits)

# Cambiamos el nombre de los factores para un mejor análisis.
dataCredit$statusAccount <- revalue(dataCredit$statusAccount, c("A11" = "<0", "A12" = "<200", "A13" = ">200","A14" = "noChkAcc"))

dataCredit$creditHistory <- mapvalues(dataCredit$creditHistory, from = c("A30", "A31", "A32","A33" ,"A34"  ), 
                                              to = c("veryGood", "good", "poor","bad" ,"critical" ))

dataCredit$statusAccount <- mapvalues(dataCredit$statusAccount, from = c("A11", "A12", "A13","A14" ), 
                                             to = c("<0", "<200", ">200", "none"))

dataCredit$creditHistory <- mapvalues(dataCredit$creditHistory, from = c("A30", "A31", "A32","A33" ,"A34"  ), 
                                              to = c("veryGood", "good", "poor","bad" ,"critical" ))

dataCredit$purpose <- mapvalues(dataCredit$purpose, from = c("A40", "A41","A42", "A43", "A44", "A45", "A46", "A47", "A48", "A49","A410"), 
                                        to = c("carNew", "carOld", "furniture","tv", "domestic", "repairs", "education", "vacation", "retraining","business","others"))

dataCredit$savingAccount <- mapvalues(dataCredit$savingAccount, from = c("A61", "A62", "A63","A64", "A65" ),
                                              to = c("x<100", "100<x<500", "500<x<1000","x>1000", "unknown"))

dataCredit$job <- mapvalues(dataCredit$job, from = c("A171", "A172", "A173", "A174" ), 
                                    to = c("unemployed", "unskilled", "skilled", "highQualified" ))

```

Mostraremos un primer análisis mediante la función summary.  
```{r message= FALSE, warning=FALSE}
# Mostramos el sumario. 
summary(dataCredit)

```
Nos hacemos una primera idea como se distribuyen las variables continuas y el número de etiquetas de los atributos cualitativos. 
Como datos relevantes observamos:

*statusAccount* - No contiene observaciones por encima de los 200.

*durationMonth* - La media de los creditos están por debajo de los 2 años.

*creditHistory* - No contiene hitórico para las etiquetas "good" y "verygood".

*otherCredits* - La mayoría solo tiene un credito. 

*purpose* - La mayoria de los creditos se pidieron para comprar un coche nuevo seguido de electrodomesticos o aparatos del hogar.

*savingAccoun* - Los ahorros están por debajo de los 100K. Posiblemente no tenga sentido pedir un credito cuando tus ahorros están por encima de los 100K.

*rateIncome* - Oscila entre 1 % - 4 %

*job* - No hay muestras para gente desempleada algo que tiene sentido ya que es un requisito para los bancos a la hora de darte un crédito.

*age* - Edad media de 33. Podría ser una edad en la que no tienes suficientes ahorros debido a una vida laboral todavía corta.

Analizamos los atributos cuantitativos en relacion a "default".
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Creámos una página nueva
grid.newpage()

# plotDurationMonth
plotDurationMonth <- dataCredit %>% 
  ggplot(aes(durationMonth, fill=default)) +
    geom_boxplot()
plotDurationMonth2 <- dataCredit %>% 
  ggplot(aes(durationMonth, fill=default)) +
    geom_bar()

# plotOtherCredits
plotOtherCredits <- dataCredit %>%
  ggplot(aes(otherCredits, fill=default)) +
    geom_bar()

# plotRateIncome
plotRateIncome <- dataCredit %>%
  ggplot(aes(rateIncome, fill=default)) +
    geom_bar()
#plotAge
plotAge2 <- dataCredit %>%
  ggplot(aes(age, fill=default)) +
    geom_boxplot()
plotAge <- dataCredit %>%
  ggplot(aes(age, fill=default)) +
    geom_bar()

grid.arrange(plotDurationMonth, plotDurationMonth2,plotOtherCredits,plotRateIncome,plotAge2, plotAge, ncol=2)

```

Análisis de los puntos más relevantes de los atributos mostrados arriba respecto "default".

*durationMonth* - La media de los creditos están por debajo de los 2 años y donde se concentra la mayoria de impagos. La distribución tiene bastantes picos debido a la frecuencia, posiblemente la duración de los creditos son fijos y rara vez se salgan del tiempo de amortizacion estandar. La media en este caso de los impagos es ligeramente superior.

*otherCredits* - La mayoría de los impagos están en clientes que solo tienen uno o dos. Lo que es una cosa logica por la proporción y tambien ya que las condiciones del banco a aceptar los creditos son más restrictivas por lo que llevarían menos riesgo. 

*rateIncome* - a mayor porcentaje más riesgo. Se confirma algo también logico, que los creditos se pagan con el sueldo. 

*age* - El número mayor de impagos en edad es ligeramente menor.

Ahora analizamos los atributos cualitativos
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Creámos una página nueva
grid.newpage()

# plotStatusAccount
plotStatusAccount <- dataCredit %>% 
  ggplot(aes(statusAccount, fill=default)) +
    geom_bar()+
    scale_x_discrete(labels = abbreviate)

# plotcreditHistory
plotCreditHistory <- dataCredit %>%
  ggplot(aes(creditHistory, fill=default)) +
    geom_bar()+
    scale_x_discrete(labels = abbreviate)

# plotpurpose
plotPurpose <- dataCredit %>%
  ggplot(aes(purpose, fill=default)) +
    geom_bar()+
    scale_x_discrete(labels = abbreviate)

#plotsavingAccount
plotSavingAccount <- dataCredit %>%
  ggplot(aes(savingAccount, fill=default)) +
    geom_bar()+
    scale_x_discrete(labels = abbreviate)

#plotJob
plotJob <- dataCredit %>%
  ggplot(aes(job, fill=default)) +
    geom_bar()+
    scale_x_discrete(labels = abbreviate)

grid.arrange(plotStatusAccount,plotCreditHistory,plotPurpose,plotSavingAccount,plotJob, ncol=2)

```

Tablas de contingencia.
```{r}
# tableStatusAccount
tableStatusAccount <- table(dataCredit$statusAccount, dataCredit$default)
tableStatusAccount
prop.table(tableStatusAccount, margin = 1)
```
```{r}
# tableCreditHistory
tableCreditHistory <- table(dataCredit$creditHistory, dataCredit$default)
tableCreditHistory
prop.table(tableCreditHistory, margin = 1)
```
```{r}
# tablePurpose
tablePurpose <- table(dataCredit$purpose, dataCredit$default)
tablePurpose
prop.table(tablePurpose, margin = 1)
```

```{r}
# tableSavingAccount
tableSavingAccount <- table(dataCredit$savingAccount, dataCredit$default)
tableSavingAccount
prop.table(tableSavingAccount, margin = 1)
```
```{r}
#tableJob
tableJob <- table(dataCredit$job, dataCredit$default)
tableJob
prop.table(tableJob, margin = 1)
```
```{r echo=TRUE, message=FALSE, warning=FALSE}

plot(tableStatusAccount, col = c("black","#008000"), main = "StatusAccount vs. Default")
plot(tableCreditHistory, col = c("black","#008000"), main = "CreditHistory vs. Default")
plot(tablePurpose, col = c("black","#008000"), main = "Purpose vs. Default")
plot(tableSavingAccount, col = c("black","#008000"), main = "SavingAccount vs. Default")
plot(tableJob, col = c("black","#008000"), main = "Job vs. Default")

```

Análisis de los puntos más relevantes de los atributos representados en las gáficas más las tablas de contingencia.

*stausAccount* - Hay una tendencia clara que a menos ahorros más riesgo de impago.

*creditHistory* - Hay más impagos en clientes con un mal o muy mal historial. Posiblemente la gente con un historial critico no recibe tantos creditos de ahí que en proporción sea menor.

*savingAccount* -  la  proporción de créditos educación tiene un porcentaje alto de impago aunque no tiene un volumen alto en el número de créditos.

*savingAccount* - La mayoría de clientes está por debajo de los 100M de ahorro que es donde hay el mayor porcentaje de impagos.

*job* - Elt ipo de trabajo en relación al impago tiene un porcentaje muy similar.


# Árboles de decisión.
 
## Árbol de decisión test-1 
Entrenamos nuestro modelo utilizando observaciones aleatorias del dataset. En este caso vamos a utilizar el 80% de observaciones para entrenar el modelo y 20% para testearlo. El subconjuto de observaciones de testeo y entrenamiento lo guardamos en las variables correspondientes.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Numero total de observaciones.
n <- nrow(dataCredit)

# Seleccionamos el 80%.
nTrain <- round(0.80 * n) 

# Creamos un vector con indices aleatorios.
set.seed(123)
trainIndices <- sample(1:n, nTrain)

# Subset training.
dataTrainX <- dataCredit[trainIndices, 1:9]  
dataTrainY <- dataCredit[trainIndices, 10]   
  
# Subset testing.
dataTestX <- dataCredit[-trainIndices, 1:9]  
dataTestY <- dataCredit[-trainIndices, 10] 

```

No mostramos el modelo gráficamente por tener demasiadas variables. Aunque se puede ver igualmente en formato texto.
```{r}
# donde se pueden ver las reglas de decisión.
model <- C50::C5.0(dataTrainX, dataTrainY)
summary(model)

#plot(model)
```
Vemos que este método clasifica erroneamente un 18.4% de las observaciones y también nos muestra las contribuciones de cada atributo en porcentaje "Attribute usage".

```{r}

# Reglas de decisión.
model <- C50::C5.0(dataTrainX, dataTrainY, rules=TRUE)
summary(model)

```
Este modelo nos ha generado 12 reglas de decisión.

Analizamos una de las ramas más largas. La regla numero 9 clasifica a un cliente de riesgo con un 73.7% si su saldo actual en depósito de cheques es inferiora 0, si tiene una hipoteca superior a 15 meses, si tiene un historial negativo, si el crédito es usado para la compra de electrodomesticos o aparatos del hogar y si tiene unos ahorros que oscilan entre 0 y 500 para para clientes con edad superior a 37.  

  Rule 9: (17/4, lift 2.6)
  
  statusAccount = <0
	
	durationMonth > 15
	
	creditHistory = poor
	
	purpose = tv
	
	savingAccount in {x<100, 100<x<500}
	
	age <= 37
	
	->  class 1  [0.737]

### Validación del modelo
```{r echo=TRUE, message=FALSE, warning=FALSE}
predicted_model <- predict( model, dataTestX, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == dataTestY) / length(predicted_model)))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
if(!require(gmodels)){
    install.packages('gmodels', repos='http://cran.us.r-project.org')
    library(gmodels)
}
```

Los valores que han tenido una prediccion positiva se encuentran en la diagonal de la matriz siendo estos 117 de 161 para 0 (sin riesgo de impago) y de 26 de un total the 39 para el valor 1 (riesgo de impago).
```{r echo=TRUE, message=FALSE, warning=FALSE}
CrossTable(dataTestY, predicted_model,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Risk', 'Prediction'))
```

## Árbol de decisión test-2 

Quiero comprobar como afecta la calidad del modelo si acorto el número de atributos. En este caso solo selectiono los que más contribuyen según el árbol anterior. Estos atributos son los siguientes: statusAccount, durationMonth, creditHistory, savingAccount. Luego, repito todos los pasos anteriores.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Seleccionamos los atributos a trabajar.
dataCredit1 <- dataCredit %>%
  select(statusAccount, durationMonth, creditHistory, savingAccount, default)

# Numero totaimpago en relación al l de observaciones.
n <- nrow(dataCredit1)

# Seleccionamos el 80%.
nTrain <- round(0.80 * n) 

# Creamos un vector con indices aleatorios.
set.seed(222)
trainIndices <- sample(1:n, nTrain)

# Subset training.
dataTrainX <- dataCredit1[trainIndices, 1:4]  
dataTrainY <- dataCredit1[trainIndices, 5]   
  
# Subset testing.
dataTestX <- dataCredit1[-trainIndices, 1:4]  
dataTestY <- dataCredit1[-trainIndices, 5] 

```

Mostramos el summario del modelo.
```{r}
# donde se pueden ver las reglas de decisión.
model <- C50::C5.0(dataTrainX, dataTrainY)
summary(model)

#plot(model)
```
Vemos que este método clasifica erroneamente un 22% en vez de un 18.4% del caso anterior.  

El modelo genera menos reglas, ya que tiene menos atributos.
```{r}
# Reglas de decisión.
model <- C50::C5.0(dataTrainX, dataTrainY, rules=TRUE)
summary(model)

#model <- C50::C5.0(dataTrainX, dataTrainY)
#plot(model)
```
Este modelo nos ha generado 8 reglas de decisión.

Y sacamos la siguiente información. Para los siguiente casos serían operaciones sín riesgo.

* Si la persona no tiene disponibilidad de hacer queques (87%).
* si tiene ahorros por encima de los 500 (82%)
* Si tiene un historialnegativo o muy malo no representa un problema de riesgo porque posiblemente no se le conceda un credito (71%). 

## Validación del modelo
```{r echo=TRUE, message=FALSE, warning=FALSE}
predicted_model <- predict( model, dataTestX, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == dataTestY) / length(predicted_model)))
```

Acabmos de comprobar como este segundo modelo compuesto por menos atributos y que tiene más bias (error del training data) que el primero y sin embargo tienes una menor varianza o sea una mejor respuesta con el subconjunto de datos de testeo. Esto es debido a que el primer modelo posiblemente tenga overfitting. 
```{r echo=TRUE, message=FALSE, warning=FALSE}
if(!require(gmodels)){
    install.packages('gmodels', repos='http://cran.us.r-project.org')
    library(gmodels)
}
```

Los valores que han tenido una predicción positiva se encuentran en la diagonal de la matriz siendo estos 117 de 161 para 0 (sin riesgo de impago) y de 26 de un total the 39 para el valor 1 (riesgo de impago).
```{r echo=TRUE, message=FALSE, warning=FALSE}
CrossTable(dataTestY, predicted_model,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Risk', 'Prediction'))
```

## Árbol de decisión test-3

En este caso puebo una variante de árbol de decisión de la libreria "rpart" el cual separa las ramas usando el método Gini. Este método tambien realiza 10-cross-validation del conjunto del dataset.

Los paso son muy parecidos al de C50 con la unica dicerencia en la preparación de los datos que serán usados como parametros en la función "rpart".
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Seleccionamos los atributos a trabajar.
dataCredit2 <- dataCredit %>%
  select(statusAccount, durationMonth, creditHistory, savingAccount, default)

# Numero totaimpago en relación al l de observaciones.
n <- nrow(dataCredit2)

# Seleccionamos el 80%.
nTrain <- round(0.80 * n) 

# Creamos un vector con indices aleatorios.
set.seed(333)
trainIndices <- sample(1:n, nTrain)

# Subset training.
dataTrainX <- dataCredit2[trainIndices, ]  

# Subset testing.
dataTestX <- dataCredit2[-trainIndices, ]  

```

Una vez he preparado los subconjunto de datos para el entrenamiento y el testeo puedo generar el modelo indicando la separación por Gini.
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Train the model (to predict 'default')
#install.packages('rpart')
library(rpart)

model <- rpart(formula = default ~ ., 
                      data = dataTrainX, 
                      method = "class",
                      parms = list(split = "gini"))

# Mostramos el modelo                      
print(model)

#str(dataTrainX)

```
Genera un árbol de 15 nodos. Vemos que el corte y la posicion de los atributos es diferente en la jerarquia del árbol usado en el método C50. 

La cálidad del modelo (77%) es muy similar al anterior (77.5%), por lo que se pueden generar árboles con estructuras diferente dando resultado muy parecidos. 
```{r echo=TRUE, message=FALSE, warning=FALSE}
#install.packages("caret")
#install.packages("e1071")

# Generamos la predicción usand el modelo.
classPrediction <- predict(object = model,  
                        newdata = dataTestX,   
                        type = "class")  
                            
# Calculamos la "confusion matrix".
caret::confusionMatrix(data = classPrediction,       
                reference = dataTestX$default) 
```

******
# Rúbrica
******
* 15% Se explica de forma clara la base de datos seleccionada y la razón de su elección.
* 10% Hay un estudio sobre los datos de los que se parte y los datos son preparados correctamente.
* 20% Se aplica un árbol de decisión de forma correcta y se obtiene una estimación del error.
* 5% Se muestra de forma gráfica el árbol obtenido.
* 10% Se explican las reglas que se obtienen.
* 10% Se usa el modelo para predecir con muestras no usadas en el entrenamiento y se obtiene una estimación del error.
* 15% Se prueba otro modelo de árbol o variantes diferentes del C50 obteniendo mejores resultados.	
* 5% Se presenta el código y es fácilmente reproducible.
* 10% Se presenta unas conclusiones donde se expone el conocimiento adquirido tras el trabajo realizado.

