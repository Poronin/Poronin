---
title: 'Minería de datos: PEC2 - Métodos no supervisados'
author: "Leonardo Segovia Vilchez"
date: "Abril 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
******
# Introducción
******
## Presentación
Esta Prueba de Evaluación Continuada cubre princpalmente los módulos 5 i 6 (Métodos de agregación y Algoritmos de asociación) del programa de la asignatura.

## Competencias
Las competencias que se trabajan en esta prueba son:  

* Uso y aplicación de las TIC en el ámbito académico y profesional.
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologias de comunicaciones actuales y emergentes así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologias de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.

## Objetivos
En esta PEC trabajaremos la generación, interpretacion y evaluación de un modelo de agregación y de un modelo donde generaremos reglas de asociación con el software de practicas. No perderemos de vista las fases de preparación de los datos, calidad del modelo y extracción inicial del conocimiento.

## Descripción de la PEC a realizar

## Recursos Básicos
**Material docente proporcionado por la UOC.** 

Módulo 5 y 6 del material didáctico.

## Criterios de valoración

**Ejercicios teóricos** 

Todos los ejercicios deben ser presentados de forma razonada y clara, especificando todos y cada uno de los pasos que se hayan llevado a cabo para su resolución. No se aceptará ninguna respuesta que no esté claramente justificada.

**Ejercicios prácticos** 

Para todas las PEC es necesario documentar en cada apartado del ejercicio práctico que se ha hecho y cómo se ha hecho.

## Formato y fecha de entega
El formato de entrega es: usernameestudiant-PECn.html/doc/docx/odt/pdf  
Fecha de Entrega: 22/04/2020  
Se debe entregar la PEC en el buzón de entregas del aula  

## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en qué se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Ejemplo 1.1
## Métodos de agregación con datos autogenerados
******
En este ejemplo vamos a generar un conjunto de muestras aleatorias para posteriormente usar el algortimo kmeans para agruparlas. Se crearán las muestras alrededor de dos puntos concretos. Por lo tanto, lo lógico será agrupar en dos clústers. Puesto que inicialmente, en un problem real, no se conoce cual es el número correcto de clústers k, vamos a probar primero con dos (el valor óptimo) y posteriormente con 4 y 8 clústers. Para evaluar la calidad de cada proceso de agrupación vamos a usar la silueta media. La silueta de cada muestra evalúa como de bien o mal está clasificada la muestra en el clúster al que ha sido asignada. Para ello se usa una fórmula que tiene en cuenta la distancia a las muestras de su clúster y la distancia a las muestras del clúster vecino más cercano. 

A la hora de probar el código que se muestra, es importante tener en cuenta que las muestras se generan de forma aleatoria y también que el algortimo kmeans tiene una inicialización aleatoria. Por lo tanto, en cada ejecución se obtendrá unos resultados ligeramente diferentes.

Lo primero que hacemos es cargar la libreria cluster que contiene las funciones que se necesitan

```{r message= FALSE, warning=FALSE}
library(cluster)
```
Generamos las muestras de forma aleatoria tomando como centro los puntos [0,0] y [5,5].
```{r message= FALSE, warning=FALSE}
n     <- 150 # número de muestras
p     <- 2   # dimensión

sigma <- 1          # varianza de la distribución
mean1 <- 0          # centro del primer grupo
mean2 <- 5          # centro del segundo grupo
n1    <- round(n/2) # número de muestras del primer grupo
n2    <- round(n/2) # número de muestras del segundo grupo

x1 <- matrix(rnorm(n1*p,mean=mean1,sd=sigma),n1,p)
x2 <- matrix(rnorm(n2*p,mean=mean2,sd=sigma),n2,p)
```

Juntamos todas las muestras generadas y las mostramos en una gráfica
```{r message= FALSE, warning=FALSE}
x  <- rbind(x1,x2)
plot (x)
```
Como se puede comprobar las muestras están claramente separadas en dos grupos. Si se quiere complicar el problema se puede modificar los puntos centrales (mean1 y mean2) haciendo que esten más próximos y/o ampliar la varianza (sigma) para que las muestras estén más dispersas.

A continuación vamos a aplicar el algoritmo kmeans con 2, 4 y 8 clústers
```{r message= FALSE, warning=FALSE}
fit2       <- kmeans(x, 2)
y_cluster2 <- fit2$cluster

fit4       <- kmeans(x, 4)
y_cluster4 <- fit4$cluster

fit8       <- kmeans(x, 8)
y_cluster8 <- fit8$cluster
```
Las variables y_cluster2, y_cluster4 e y_cluster8 contienen para cada muestra el identificador del clúster a las que han sido asignadas. Por ejemplo, en el caso de los k=2 las muestras se han asignado al clúster 1 o al 2

```{r message= FALSE, warning=FALSE}
y_cluster2
```

Para visualizar los clústers podemos usar la función clusplot. Vemos la agrupación con 2 clústers
```{r message= FALSE, warning=FALSE}
clusplot(x, fit2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

con 4
```{r message= FALSE, warning=FALSE}
clusplot(x, fit4$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

y con 8
```{r message= FALSE, warning=FALSE}
clusplot(x, fit8$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```
También podemos visualizar el resultado del proceso de agrupamiento con el siguiente código para el caso de 2 clústers
```{r message= FALSE, warning=FALSE}
plot(x[y_cluster2==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster2==2,],col='red')
```

para 4
```{r message= FALSE, warning=FALSE}

plot(x[y_cluster4==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster4==2,],col='red')
points(x[y_cluster4==3,],col='green')
points(x[y_cluster4==4,],col='black')
```

y para 8
```{r message= FALSE, warning=FALSE}
plot(x[y_cluster8==1,],col='blue', xlim=c(min(x[,1]), max(x[,1])), ylim=c(min(x[,2]), max(x[,2])))
points(x[y_cluster8==2,],col='red')
points(x[y_cluster8==3,],col='green')
points(x[y_cluster8==4,],col='black')
points(x[y_cluster8==5,],col='yellow')
points(x[y_cluster8==6,],col='purple')
points(x[y_cluster8==7,],col='cyan')
points(x[y_cluster8==8,],col='orange')
```

Ahora vamos a evaluar la calidad del proceso de agregación. Para ello usaremos la función silhouette que calcula la silueta de cada muestra

```{r message= FALSE, warning=FALSE}
d  <- daisy(x) 
sk2 <- silhouette(y_cluster2, d)
sk4 <- silhouette(y_cluster4, d)
sk8 <- silhouette(y_cluster8, d)
```

La función silhouette devuelve para cada muestra, el clúster dónde ha sido asignado, el clúster vecino y el valor de la silueta. Por lo tanto, calculando la media de la tercera columna podemos obtener una estimación de la calidad del agrupamiento

```{r message= FALSE, warning=FALSE}
mean(sk2[,3])
mean(sk4[,3])
mean(sk8[,3])
```

Como se puede comprobar, agrupar con dos clúster es mejor que en 4 o en 8, lo cual es lógico teniendo en cuenta como se han generado los datos.

******
# Ejemplo 1.2
## Métodos de agregación con datos reales 
******

A continuación vamos a ver otro ejemplo de cómo se usan los modelos de agregación. Para ello usaremos el fichero iris.csv. Esta base de datos se encuentra descrita en https://archive.ics.uci.edu/ml/datasets/iris. Este dataset está previamente trabajado para que los datos esten limpios y sin errores. De no ser así antes de nada deberíamos buscar errores, valores nulos u outlayers. Deberíamos mirar de discretizar o eliminar columnas. Incluso realizar este último paso varias veces para comprobar los diferentes resultados y elegir el que mejor performance nos dé.
De todas formas vamos a visualizar la estructura y resumen de los datos
```{r message= FALSE, warning=FALSE}
iris_data<-read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", header=T, sep=",")
attach(iris_data)
colnames(iris_data) <- c("sepalLength", "sepalWidth", "petalLength", "petalWidth", "class")
summary(iris_data)

```

Como se puede comprobar, esta base de datos está pensada para problemas de clasificación supervisada que pretende clasificar cada tipo de flor en uno de las tres clases existentes (Iris-setosa, Iris-versicolor o Iris-virginica). Como en este ejemplo vamos a usar un método no supervisado, transformaremos el problema supervisado original en uno no supervisado. Para consegirlo no usaremos la columna class, que es la variable que se quiere predecir. Por lo tanto, intentaremos encontrar agrupaciones usando únicamente los cuatro atributos que caracterizan a cada flor.
 
Cargamos  los datos y nos quedamos unicamente con las cuatro columnas que definen a cada flor
```{r message= FALSE, warning=FALSE}
x <- iris_data[,1:4]
```

Como inicialmente no conocemos el número óptimo de clústers, probamos con varios valores
```{r message= FALSE, warning=FALSE}
d <- daisy(x) 
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(x, i)
  y_cluster     <- fit$cluster
  sk            <- silhouette(y_cluster, d)
  resultados[i] <- mean(sk[,3])
}
```


Mostramos en un gráfica los valores de las siluetas media de cada prueba para comprobar que número de clústers es el mejor
```{r message= FALSE, warning=FALSE}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Numbero de clusters",ylab="Silueta")
```

Aunque el valor esperado es k=3, dado que el conjunto original tiene 3 clases, el mejor valor que se obtiene es k=2.

Otro forma de evaluar cual es el mejor número de clústers es considerar el mejor modelo, aquel que ofrece la menor suma de los cuadrados de las distancias de los puntos de cada grupo con respecto a su centro (withinss), con la mayor separación entre centros de grupos (betweenss). Como se puede comprobar es una idea conceptualmente similar a la silueta. Una manera común de hacer la selección del número de clústers consiste en aplicar el método elbow (codo), que no es más que la selección del número de clústers en base a la inspección de la gráfica que se obtiene al iterar con el mismo conjunto de datos para distintos valores del número de clústers. Se seleccionará el valor que se encuentra en el "codo" de la curva

```{r message= FALSE, warning=FALSE}
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(x, i)
  resultados[i] <- fit$tot.withinss
}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Número de clusters",ylab="tot.tot.withinss")
```

En este caso el número óptimo de clústers son 4 que es cuando la curva comienza a estabilizarse.

También se puede usar la función kmeansruns del paquete fpc que ejecutá el algoritmo kmeans con un conjunto de valores y selecciona el valor del número de clústers que mejor funcione de acuerdo a dos criterios la silueta media ("asw") y Calinski-Harabasz ("ch").

```{r message= FALSE, warning=FALSE}
library(fpc)
fit_ch  <- kmeansruns(x, krange = 1:10, criterion = "ch") 
fit_asw <- kmeansruns(x, krange = 1:10, criterion = "asw") 
```

Podemos comprobar el valor con el que se ha obtenido el mejor resultado y también mostrar el resultado obtenido para todos los valores de k usando ambos criterios

```{r message= FALSE, warning=FALSE}
fit_ch$bestk
fit_asw$bestk

plot(1:10,fit_ch$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio Calinski-Harabasz")
plot(1:10,fit_asw$crit,type="o",col="blue",pch=0,xlab="Número de clústers",ylab="Criterio silueta media")

```

Los resultados son muy parecidos a los que hemos obtenido anteriormente. Con el criterio de la silueta media se obtienen dos clústers y con el Calinski-Harabasz se obtienen 3.

Como se ha comprobado, conocer el número óptimo de clústers no es un problema fácil. Tampoco lo es la evaluación de los modelos de agregación.

Como en el caso que estudiamos sabemos que los datos pueden ser agrupados en 3 clases, vamos a ver como se ha comportado el kmeans en el caso de pedirle 3 clústers. Para eso comparamos visualmente los campos dos a dos, con el valor real que sabemos está almacenado en el campo "class" del dataset original.
```{r message= FALSE, warning=FALSE}
iris3clusters <- kmeans(x, 3)

# sepalLength y sepalWidth
plot(x[c(1,2)], col=iris3clusters$cluster)
plot(x[c(1,2)], col=iris_data$class)
```

Podemos observar que el sépalo no es un buen indicador para diferenciar a las tres subespecies, no con la metodología de kmeans, dado que dos de las subespecies estan demasiado mezcladas para poder diferenciar nada.
```{r message= FALSE, warning=FALSE}
# petalLength y petalWidth
plot(x[c(3,4)], col=iris3clusters$cluster)
plot(x[c(3,4)], col=iris_data$class)
```

 El tamaño del pétalo sin embargo, parece hacer un mucho mejor trabajo para dividir las tres clases de flores. El grupo formado por los puntos negros que ha encontrado el algoritmo coincide con los de la flor Iris Setosa. Los otros dos grupos sin embargo se entremezclan algo más, y hay ciertos puntos que se clasifican como Versicolor cuando en realidad son Virginica.
 
 Una buena técnica que ayuda a entender los grupos que se han formado, es mirar de darles un nombre. Cómo por ejemplo:
 
 - Grupo 1: Sólo setosas
 - Grupo 2: Principalmente versicolor
 - Grupo 3: Virgínicas o iris pétalo grande
 
 Esto nos ayuda a entender cómo están formados los grupos y a referirnos a ellos en análisis posteriores.
 
Una última cosa que nos queda por hacer, es saber cuales de las muestras iniciales han sido mal clasificadas y cómo. Eso lo conseguimos con el siguiente comando.

```{r message= FALSE, warning=FALSE}
table(iris3clusters$cluster,iris_data$class)
```

Y así, podemos sacar un porcentaje de precisión del modelo
```{r message= FALSE, warning=FALSE}
100*(36 + 48 + 49)/(133+(2+14))
```

## Ejercicio 1.1
Tomando como punto de partida los ejemplos mostrados, realizar un estudio similar con otro conjunto de datos. Pueden ser datos reales de vuestro ámbito laboral o de algún repositorio de datos de Internet. Mirad por ejemplo: http://www.ics.uci.edu/~mlearn/MLSummary.html.

A la hora de elegir la base de datos ten en cuenta que sea apropiada para problemas no supervisados y que los atributos sean también apropiados para su uso con el algoritmo kmeans.

No hay que olvidarse de la fase de preparación y análisis de datos.

### Respuesta 1.1:

Voy a trabajar un conjunto de datos sobre vinos con el objetivo de poder encontrar algun tipo de agrupación natural y así luego poder analizar más en detalle algunos atributos en especifico respecto a los grupos encontrados.

Cargamos las librerías necesarias para el análisis.

```{r message= FALSE, warning=FALSE}

# install.packages('dendextend')
# installed.packages("Dplyr")
library(tibble)
library(dplyr)
library(ggplot2)
library(arules)
library(purrr)
library(dendextend)

```

Leemos el archivo del conjunto de datos y lo guardamos en un dataframe para trabajar con él. Mostramos tambien las cinco primeras lineas para ver que tipo de datos contiene y un sumario de cada atributo. Este primer análisi ya nos indica que el escalado es necesario.

```{r message= FALSE, warning=FALSE}

# Leémos el archivo del repositorio
wineRead <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", header=T, sep=",")

# Comprobamos que los datos se habían importado con una columna sin infomación.
wineData <- wineRead[,2:14]

# Añadimos la base de datos a la "search path" de R
attach(wineData)

# # Añadimos el nombre de las columnas
colnames(wineData) <- c("Alcohol","MalicAcid","Ash","AlcalinityOfAsh","Magnesium","TotalPhenols","Flavanoids","NonflavanoidPhenols","Proanthocyanins","ColorIntensity","Hue","OD280OD315","Proline")

# Evaluamos posible valores NA
# Comprobamos si los valores no son compatibles los unos con los otros y si hay que escalarlos
# Comprobamos si hay valores categoricos
head(wineData, 5)
summary(wineData)
str(wineData)
```

Data Set Information:

These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.


The attributes are:
1) Alcohol
2) Malic acid
3) Ash
4) Alcalinity of ash
5) Magnesium
6) Total phenols
7) Flavanoids
8) Nonflavanoid phenols
9) Proanthocyanins
10)Color intensity
11)Hue
12)OD280/OD315 of diluted wines
13)Proline 

#### Inspección de los datos

Conprobamos la integridad de los datos.
```{r message= FALSE, warning=FALSE}

# Confimamos que están todos los valores 
colSums(wineData == " " | wineData == "" | is.na(wineData))
```
Procedemos al escalado de los datos para poder relacionar atributos entre sí.

```{r message= FALSE, warning=FALSE}

# Escalamos los el conjunto de datos
wineDataScaled <- scale(wineData)

# Comprobamos el resultado del escalado
head(wineDataScaled)
```

El escalado por defecto se hace con respecto a la media centrada en 0 y la SD en 1 y -1.

```{r message= FALSE, warning=FALSE}

# Escalado
plot(wineDataScaled)

```

### Busqueda del valor de K

Antes de trabajar el algoritmo de k-means directamente quería estar seguro del mejor valor de k. Así que he seguido un camino un poco diferente,  previamente al Kmeans he utilizado diferentes algoritmos y técnicas para encontrar k que más se ajusta. Este proceso lo he descrito en el ejercicio siguiente 1.2 en el he utilizado las siguientes tecnicas:

Hierarchical clustering

Elbow análisis

Análisis de la silueta utilizando el algoritmo PAM


Sugiero saltar al ejercicio 1.2 ya que son los pasos logicos que he seguido durante el proceso y volver a este punto una vez obtenido el mejor valor de k.   


### K-means con k=2 y conclusiones


```{r message= FALSE, warning=FALSE}

# Aplicamos el algoritmo Kmeans
kWine3 <- kmeans(wineDataScaled, center = 3)

# Convertimos el array en un data frame para poder visualizarlo con la libreria Ggplo2
kWine3Df <- as.data.frame(wineDataScaled)

# Añadimos la columna con los clusters
kWine3Df <- mutate(kWine3Df, cluster = kWine3$cluster)

# Gráfica Alcohol y Tono
ggplot(kWine3Df, aes(x = Alcohol, y = Hue, color = factor(cluster) )) +
  geom_point()

# Gráfica Alcohol y la intensidad de color
ggplot(kWine3Df, aes(x = Alcohol, y = ColorIntensity, color = factor(cluster))) +
  geom_point()

```

Nota: Comprobar el cluster en el archivo rmd ya que al generar el html los colores y la numeración de los clusters puede cambiar. 

Color intensity:

El cluster verde contiene menos alcohol que la media y se mueve entre dos desviaciones estandar por debajo de la media. Tiene una intensidad de color bastante parecida en todo su grupo, así que parece ser que la diferencia de alcohol no afecta a la intensidad decolor especialmente. La intensidad de  color es bastante baja y parece estar centrada en -SD del eje y.

El cluster rojo y azul están más entremezclados entre sí. El cluster azul en especial es más disperso. Ambos tiene un nivel de alcohol parecido entre ellos, siendo el cluster rojo el que esta más desplazado ligeramente a la derecha de la distibución. El cluster azul es el que tiene el color más intenso.

Según lo analizado podemos decir que el color es más intenso cuando los grados de alcohol aumentan.

Tono:

A primera vista no vemos la misma relación que con el alchohola y intensidad del color, ya que el cluster rojo y verde tienen más o menos el mismo tono para distinto rango de alcohol.

El cluster rojo y verde tiene colores más puros. 
El cluster azul tiene menos tono por lo que será levemente un color menos vivo (más cerca de la escala de grises)


## Ejercicio 1.2
Buscar información sobre otros métodos de agregación diferentes al Kmeans. Partiendo del ejercicio anterior probar el funcionamiento de al menos 2 métodos diferentes y comparar los resultados obtenidos.

### Respuesta 1.2

Busqueda de K mediante hierarchical clustering

### Hierarchical clustering

Teniendo los datos ya escalados del apartado anterior podemos proceder a generar la matriz de distancia.
```{r message= FALSE, warning=FALSE}

# Calculamos la distancia entre observaciones mediante la método "euclidean" (Distancia Euclidiana)
wineDist <- dist(wineDataScaled, method = "euclidean")
```

Una vez hemos calculado la matriz de distancias para todas las observaciones podemos crear el cluster jerárquico. Hay diferentes métodos para unir las obserciones o grupo de observaciones entre sí y así poder ir formando, con las observaciones, clusters jerárquicos. 

- Average, hace la media de las observaciones del grupo.

- Complete, seleccina la observación más alejada, la distancia máxima posible. 

- Single, seleccina la observación más cercana, la distancia minima posible. 

Analizo los dedrogramas y veo que método me da mejor agrupación.

Método por "average":

```{r message= FALSE, warning=FALSE}
# Método por "average"
wineAvg <- hclust(wineDist, method = "average")
plot(wineAvg)
rect.hclust(wineAvg , k = 6, border = 2:2)

# Otra forma de colorear los posibles clusters
#dendWineAvg <- as.dendrogram(wineAvg)
#dendColorWineAvg <- color_branches(dendWineAvg, h = 3)
#plot(dendColorWineAvg)

```
He probado ha realizar el corte a diferentes alturas pero me salen cluster muy desiguales.


Método por "complete":

```{r message= FALSE, warning=FALSE}

# Método por "complete"
wineMax <- hclust(wineDist, method = "complete")
plot(wineMax)
rect.hclust(wineMax , k = 3, border = 2:6)

# Otra forma de colorear los posibles clusters
#dendWineMax <- as.dendrogram(wineMax)
#dendColorWineMax <- color_branches(dendWineMax, h = 9)
#plot(dendColorWineMax)

```
He formado tres clusters con una altura de 9. Los cluster visualmete parecen bastante parecidos en tamaño. 

Método por "single":

```{r message= FALSE, warning=FALSE}

# Método por "single"
wineMin <- hclust(wineDist, method = "single")
plot(wineMin)
rect.hclust(wineMin , k = 10, border = 2:6)

# Otra forma de colorear los posibles clusters
#dendWineMin <- as.dendrogram(wineMin)
#dendColorWineMin <- color_branches(dendWineMin, h = 3)
#plot(dendColorWineMin)

```

El resultado es incluso peor que con el método average. Así que parece que el método complete con k=3 es el mejor valor.

Realizamos el corte en k = 3 y mostramos el resultado gráficamente.
Busco la relacion que tiene el alcohol con el color y el tono.

```{r message= FALSE, warning=FALSE}

# Cortamos el jerarquía en k = 3 
cutWine <- cutree (wineMax, k = 3)

# Convertimos el array en un data frame para poder visualizarlo con la libreria Ggplo2
dfWineDataScaled <- as.data.frame(wineDataScaled)

# Adjuntamos los clusters al conjunto original
wineClustered <- mutate(dfWineDataScaled, cluster = cutWine)

# Mostramos las cabeceras para asegurarnos que los datos son correctos
head(wineClustered, 2)

# Mostramos por pantalla los atributos Alchol respecto a tono y intensidad de color
ggplot(wineClustered, aes(x = Alcohol, y = ColorIntensity, color = factor(cluster))) + geom_point()
ggplot(wineClustered, aes(x = Alcohol, y = Hue, color = factor(cluster))) + geom_point()

```

El resultado (color intensity) son tres grupos con algunas observaciones mezcladas entre grupos. Tambien vemos que el grupo 2 está mas agrupado que el 1 y 3, siendo el 3 el mas disperso.

La posicion de los cluster cambian en relación al tono aunque con similar caracteristicas, tres grupos diferenciados con algunas observaciones mezcladas entre grupos. La distancia entre gurpos no parece ser muy grande visualmente.


### Evaluación de diferentes valores de k utilizando Kmeans (Elbow análisis)

Probamos el algoritmo Kmeans para distintas k para obtener la distancia interna total de los distintos cluster(Total within-cluster). Este metodo no mide distacia de respecto a los vecinos. 
```{r message= FALSE, warning=FALSE}

# Realizamos un bucle para valores que van desde 1 a 10.
totWine <- map_dbl( 1:10, function(k) {
  model <- kmeans(wineDataScaled, center = k)
  model$tot.withinss
})

```

Procesamos los datos para poder mostrarlos mediante la libreria ggplot.

```{r message= FALSE, warning=FALSE}

# Generamos un dataframe de los datos obtenidos.
elbowWineDf <- data.frame(k = 1:10, totWine = totWine)

# Comprobamos el paso anterior mostrando las primeras lineas ...
head(elbowWineDf, 3)

# ...y lo mostramos por pantalla.
ggplot(elbowWineDf, aes(x = k, y = totWine)) + 
  geom_line() +
  scale_x_continuous( breaks = 1:10)
  
```

En la grafica se ve la logica de este método ya que cuando más cluster tenemos más la distancia disminuye. La distancia seía cero cuando K =  Numero de observaciones por lo que cada muestra sería su propio cluster.

Vemos que el codo aparece en k = 3.


### Análisisde la silueta Elbow análisis

Ahora utilizo el algoritmo pam del ingles Partitioning Around Medoids.

*En contraste con el algoritmo k-means, k-medoids escoge datapoints como centros y trabaja con una métrica arbitraria de distancias entre datapoints en vez de usar la norma l2* (véase:https://es.wikipedia.org/wiki/K-medoids)

Este metodo si que tiene en cuenta la distancia de una observacion dentro de su cluster con respecto a los cluster vecinos. El resultado que obtenemos oscila entre 1 y -1.
Entre >0 y <1 esta en el cluster correcto. 
Entre <0 y >-1 está en el cluster incorrecto ya que hay un vecino mas próximo.
0 está en la frontera entre clusters.

```{r message= FALSE, warning=FALSE}

# Realizamos un bucle para valores que van desde 2 a 10.
silWine <- map_dbl(2:10, function(k) {
  model <- pam(wineDataScaled, k = k) 
  model$silinfo$avg.width
})

```

Procesamos la medias de las distancias obtenidas para poder mostrarlos mediante la libreria ggplot.

```{r message= FALSE, warning=FALSE}
# Generamos un dataframe de los datos obtenidos.
silWineDf <- data.frame(k = 2:10, silWine = silWine)

# Comprobamos el paso anterior mostrando las primeras lineas ...
head(silWineDf, 3)

# ...y lo mostramos por pantalla.
ggplot(silWineDf, aes(x = k, y = silWine)) + 
  geom_line() +
  scale_x_continuous( breaks = 2:10)
  
```

Comprobamos que el valor más alto es para k = 3.

Observamos en que se diferencian los cluster k 2,3,4.

```{r message= FALSE, warning=FALSE}

# Silueta para k = 2
silWine3 <- pam(wineDataScaled, k = 2)
plot(silhouette(silWine3))

# Silueta para k = 2
silWine3 <- pam(wineDataScaled, k = 3)
plot(silhouette(silWine3))

# Silueta para k = 2
silWine3 <- pam(wineDataScaled, k = 4)
plot(silhouette(silWine3))

```

Vemo que le k=3 es el que tiene la media más alta. También observamos que solo tienes algunas observaciones del cluster 1 que estan más cerca a sus vecinos. Siendo este el mejor escenario de los tres.

******
# Ejemplo 2
## Métodos de asociación
******
En este ejemplo vamos trabajar el algoritmo "apriori" para obtener reglas de asociacion a partir de un data set Dichas reglas nos ayudarán a comprender cómo la información del data set se relaciona entre si.
Para dicho objetivo vamos a trabajar el dataset de Groceries, que ya viene incluido con las librerias de arules.
```{r message= FALSE, warning=FALSE}
# install.packages("arules")
library(arules)
library(purrr)
data("Groceries")
```
Inspeccionamos el dataset y vemos que tiene un listado de elementos que fueron comprados juntos. Vamos a analizarlo un poco visualmente.
```{r message= FALSE, warning=FALSE}
?Groceries
inspect(head(Groceries, 5))
```
En el siguiente plot podemos ver que los tres elementos más vendidos son la leche entera, otras verduras y bollería. Dada la simplicidad del Dataset no se pueden hacer mucho más análisis. Pero para datasets más complejos miraríamos la freqüencia y distribución de todos los campos, en busca de posibles errores.
```{r message= FALSE, warning=FALSE}
itemFrequencyPlot(Groceries,topN=20,type="absolute")
```
Si lanzamos el algoritmo "apriori", generaremos directamente un set de reglas con diferente soporte, confianza y lift. El soporte indica cuantas veces se han encontrado las reglas {lsh => rhs} en el dataset, cuanto más alto mejor. La confianza habla de la probabilidad de que {rhs} se de en función de {lhs}. Y el lift es un parámetro que nos indica cuánto de aleatoriedad hay en las reglas. Un lift de 1 o menos es que las reglas son completamente fruto del azar.
```{r message= FALSE, warning=FALSE}
grocery_rules <- apriori(Groceries, parameter = list(support = 0.01, confidence = 0.5))

inspect(head(sort(grocery_rules, by = "confidence"), 3))
```
Podemos probar a ordenar las reglas por los diferentes parámetros, para ver que información podemos obtener.
```{r message= FALSE, warning=FALSE}
inspect(head(sort(grocery_rules, by = "support"), 3))
```
ordenando por support vemos que, con un lift de 2 y una confianza del 51%, podemos decir que la gente que en la misma compra hacía verduras y yogurt, compraban también leche entera. Hay que tener en cuenta que la leche entera es por otro lado el elemento más vendido de la tienda.
```{r message= FALSE, warning=FALSE}
inspect(head(sort(grocery_rules, by = "lift"), 3))
```
Por otro lado, si ordenamos por lift, vemos que con un soporte del 1% y una confianza del 58%, la gente que compra cítricos y tubérculos compra tambien verduras

Esta información nos puede ayudar a dar consejos a la dirección de la disposición de los elementos en la tienda o de que productos poner en oferta según lo que se ha comprado. Y si tuviéramos más información podríamos hacer análisis más profundos y ver que clientes compran exactamente qué.

## Ejercicio 2.1:  

En este ejercicio seguiréis los pasos del ciclo de vida de un proyecto de minería de datos para el caso de un algoritmo de generación de reglas de asociación. Lo haréis con el fichero Lastfm.csv que encontraréis adjunto. Este fichero contiene un conjunto de registros. Estos registros son el histórico de las canciones que ha escuchado un usuario (user) en un portal Web de música. "artist" es el nombre del grupo que ha escuchado, sex y country corresponden a variables que describen al usuario.

### Respuesta 2.1:

Leemos el archivo que contiene el dataset *lastfm* y comprobamos los datos y la estructura del mismo. 

```{r message= FALSE, warning=FALSE}

# Leémos el archivo del repositorio
lastfmRead <- read.csv('lastfm.csv', header=T, sep=",")

# Conjuto de datos
lastfmData <- lastfmRead

# Mostramos la cabeceras
head(lastfmData,5)

# Observamos el dataset
lastfmData %>% glimpse()

```

#### Inspección de los datos

Comprobamos posibles valores vacíos.

```{r message= FALSE, warning=FALSE}

# Confimamos que están todos los valores 
colSums(lastfmData == " " | lastfmData == "" | is.na(lastfmData))

```

Analizamos los datos del dataset. Estamos interesados en saber si hay artistas repetidos por usuario, ya que la cantidad del mismo artista por usuarion es una informacion no relevante para este tipo de método. Vemos tambien el tipo de distribucion y la media de la misma.

```{r message= FALSE, warning=FALSE}

# Comprobamos los usuarios y artistis disponibles en el dataset
n_distinct(lastfmData$user)
n_distinct(lastfmData$artist)

# Podemos comparar por usuario si hay artistas repetidos
dfLastfmData <- lastfmData %>% 
  group_by(user)  %>%
  summarise(
    nTotal_row = n(),
    nArtist = n_distinct(artist)
  )

# Media de artistas escuchados
dfLastfmData %>%
  summarize(
    avgArtist = mean(nArtist)
  )

# Distribución de numero de artistas escuchados
lastfmData %>% 
  group_by(user)  %>%
  summarise(
    nTotal_row = n(),
    nArtist = n_distinct(artist)
  ) %>%
  ggplot(aes(nArtist)) + geom_bar() + ggtitle("Distribución de numero de artistas escuchados")

```

Ahora nos disponemos a preparar el dataset en el formato correcto antes de convertirlo en clase "transactional"

```{r message= FALSE, warning=FALSE}

# Data list de los atributos necesarios
dlLastfm = split(lastfmData$artist, lastfmData$user)

#Ccomprobamos el resultado 
head(dlLastfm,1)

```

Generamos la base de datos tipo transactional

```{r message= FALSE, warning=FALSE}

# Generamos la base de datos tipo transactional desde la data list
trxLastfm = as(dlLastfm, "transactions")

# Comprobamos el resultado
inspect(head(trxLastfm,1))

```

#### Densidad y frecuencia

Mostramos la matriz (items x transactions) de los primeros 500  usuarios para hacernos una idea de la densidad, que es igual a items por transacción (puntos negros) dividido por el numero total de posiciones de la matriz. En este caso la matriz tiene una desidad del 0.01925319 y como habiamos visto previamente tiene una distribucion centrada en el 19.33.

Tambien es relevante ver el top 5 de los artistas más reproducidos.
 radiohead
 the beatles
 coldplay
 red hot chili peppers 
 muse  

```{r message= FALSE, warning=FALSE}

# Matriz de artistas = Items y transacciones usuarios que contienen ese artista
image(trxLastfm[1:500,1:500])

# Podemos obtener un análisis muy interesante utilizando summary
summary(trxLastfm)

```

Mostramos mediante una grafica los 10 artistas más escuchados en valores absolutos.

```{r message= FALSE, warning=FALSE}

# Mostramos el top 10
itemFrequencyPlot(trxLastfm, 
                  type= "absolute",
                  topN = 10,
                  horiz = TRUE,
                  main = "Frecuencia de los artistas")

```

Extraemos los artistas más frecuentes y comprobamos que la proporción de transaciones para el artista radiohead es de 18.02%.

```{r message= FALSE, warning=FALSE}

# Aplicamos el algoritmo apriori
itemSets = apriori(trxLastfm,
                  parameter = list(support = 0.1,
                                   target = 'frequent'
                                   )
                  )

# Ordenamos y mostramos el resultado  
inspect(head(sort(itemSets, by = 'support'),5))

```

Extraemos los artistas más frecuentes para un tamaño minimo de 2 artistas por itemset y comprobamos que la proporción de transacionesque es de 5.82%. A medidad que el itemset contiene más items la confianza disminuye.

```{r message= FALSE, warning=FALSE}

# # Aplicamos el algoritmo apriori
itemSets = apriori(trxLastfm,
                  parameter = list(support = 0.01,
                                   target = 'frequent',
                                   minlen = 2
                                   )
                  )

# Ordenamos y mostramos el resultado
inspect(head(sort(itemSets, by = 'support'),5))


```

#### Applicamos Apriori para obtener las reglas

Aplicamos ahora el algoritmo para obtener todas las reglas cuando la confianza minima es 40%, apoyo es de 1% y un minimo de items por sets de 3. 
El resultado obtenido es de un total de 98 reglas.

```{r message= FALSE, warning=FALSE}

# Creamos lar reglas basandonos en los siguientes parámetros
rulesLastfm = apriori(trxLastfm,
                        parameter = list( supp = 0.01,
                                          conf = 0.40,
                                          minlen= 3,
                                          target = "rules"
                                        )
                      )

# Obtenemos 98 reglas 
inspect(head(sort(rulesLastfm, by = "confidence"),5))

# Summary
summary(rulesLastfm)

```

##### Reducción de reglas

Con los parámetros anteriores hemos obtenido 98 reglas pero en realidad hay reglas que son redundantes y podrían ser eliminadas disminuyendo el numero total de reglas sín perder información. Se define como regla redundante si existe una regla generalmayor con el mismo o mayor nivel de confianza.
En este caso pasamos de 98 reglas a 50.
```{r message= FALSE, warning=FALSE}

# obtenemos la reglas redundantes
redRulesLastfm = is.redundant(rulesLastfm)
# Eliminamos las reglas redundantes del conjunto de reglas
rulesLastfm.pruned = rulesLastfm[!redRulesLastfm]

# Obtenemos 50 reglas 
inspect(head(sort(rulesLastfm.pruned, by = "confidence"),5))
```

##### Recomendación de Coldplay

Vemos en que casos Coldplay es recomendado a un posible consumidor. Esta recomendación en concreto tiene una probabilidad media de 1.12% y una confianza que varía desde 46% hasta el 59%. Comprobamos tambien que todas las reglas tienen un valor superior a 1 en su  lift. Si el valor fuera inferior a 1 no habría una asociación entre el itemset y el artista Coldplay. Así que cuanto mayor el lift más fuerte es la asociacón.

```{r message= FALSE, warning=FALSE}

# Filtramos para coldplay como rhs
inspect(subset(rulesLastfm.pruned, subset = rhs %in% "coldplay"))

# Summary
summary(inspect(subset(rulesLastfm.pruned, subset = rhs %in% "coldplay")))

```



Fuentes:

https://www.datacamp.com/community/tutorials/hierarchical-clustering-R

https://www.datacamp.com/community/tutorials/market-basket-analysis-r

https://www.datacamp.com/community/tutorials/k-means-clustering-r




******
# Rúbrica
******

## Ejercicio 1.1

* 15%. Se explican los campos de la base de datos, preparación y análisis de datos
* 10%. Se aplica el algoritmo de agrupamiento de forma correcta.
* 25%. Se prueban con diferentes valores de k.
* 10%. Se obtiene una medida de lo bueno que es el agrupamiento.
* 10%. Se ponen nombres a las asociaciones.
* 20%. Se describen e interpretan los diferentes clústers obtenidos.
* 10%. Se presenta el código y es fácilmente reproducible.


## Ejercicio 1.2

* 25%. Se prueba un algoritmo diferente al kmeans.
* 25%. Se prueba otro algoritmo diferente al kmeans.
* 40%. Se comparan los resultados del kmeans y los otros dos métodos probados en este ejercicio.
* 10%. Se presenta el código y es fácilmente reproducible.

## Ejercicio 2.1

* 10%. Se realiza un resumen de los datos incluidos en la base de datos.
* 15%. Se preparan los datos de forma correcta.
* 10%. Se aplica el algoritmo de reglas de asociación.
* 20%. Se realizan diferentes pruebas variando algunos parámetros.
* 35%. Se explican las conclusiones que se obtienen.
* 10%. Se presenta el código y es fácilmente reproducible.
