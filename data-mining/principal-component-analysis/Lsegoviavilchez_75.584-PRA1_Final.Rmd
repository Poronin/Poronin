---
title: 'Minería de datos: PRA1 - Selección y preparación de un juego de datos'
author: "Autor: Leonardo Segovia Vilchez"
date: "Abril 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo    
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PRA1.header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

******
# Introducción
******
## Presentación
Esta práctica cubre de forma transversal la asignatura.

Las Prácticas 1 y 2 de la asignatura se plantean de una forma conjunta de modo que la Práctica 2 será continuación de la 1.

El objetivo global de las dos prácticas consiste en seleccionar uno o varios juegos de datos, realizar las tareas de **preparación y análisis exploratorio** con el objetivo de disponer de datos listos para **aplicar algoritmos** de clustering, asociación y clasificación.

## Competencias
Las competencias que se trabajan en esta prueba son:  

* Uso y aplicación de las TIC en el ámbito académico y profesional.
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.

## Objetivos
La correcta asimilación de todos los aspectos trabajados durante el semestre.  
En esta práctica abordamos un caso real de minería de datos donde tenemos que poner en juego todos los conceptos trabajados.
Hay que trabajar todo el ciclo de vida del proyecto. Desde el objetivo del proyecto hasta la implementación del conocimiento encontrado pasando por la preparación, limpieza de los datos, conocimiento de los datos, generación del modelo, interpretación y evaluación.

## Descripción de la PRA a realizar

## Recursos Básicos
Material docente proporcionado por la UOC. 

## Criterios de valoración

**Ejercicios prácticos** 

Para todas las PEC es **necesario documentar** en cada apartado del ejercicio práctico que se ha hecho y como se ha hecho.

## Formato y fecha de entrega PRA_1
El formato de entrega es: usernameestudiant-PRA1.html y RMD  
Fecha de entrega: 06/05/2020  
Se debe entregar la PRA_1 en el buzón de entregas del aula  

## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y su estatus legal: si la obra esta protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra esta protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Enunciado
******
Todo estudio de minería debe nacer de una necesidad de alcanzar un objetivo que nos marcamos y que sólo podremos obtener a través de una colección de buenas prácticas basadas en la Minería de Datos.

El mundo de la minería de datos hemos de contemplar 3 ejes:  

1. Uno de ellos es el profundo **conocimiento** que deberíamos tener **del ámbito de los datos que estamos tratando** al que intentamos dar respuestas. 

2. El otro gran eje es sin duda las **capacidades analíticas** que seamos capaces de desplegar y en este sentido, las dos prácticas de esta asignatura pretenden que el estudiante realice un recorrido sólido por este segundo eje.  

3. El tercer eje son los **Datos**. Llegar al objetivo debe concretarse con preguntas analíticas que a su vez sean viables responder a partir de los datos de que disponemos. La tarea de analizar los datos es sin duda importante, pero la tarea de identificarlos y obtenerlos es para un analista un reto permanente.  

Como **primera parte** del estudio analítico que nos disponemos a realizar, se pide al estudiante que complete los siguientes pasos:   

1.Seleccionar un juego de datos y justificar su elección. El juego de datos deberá tener capacidades para que se le puedan aplicar algoritmos supervisados, algoritmos no supervisados y reglas de asociación. Si el juego de datos no soporta los tres modelos se pueden elegir varios conjuntos de datos, derivar datos nuevos o fusionarlos con otros.  

2. Realizar un análisis exploratorio del juego de datos seleccionado.   

3. Realizar las tareas de limpieza y acondicionado necesarias para poder ser usado en procesos de creación de modelos de minería posteriores.

4. Realizar métodos de discretización

5. Aplicar un estudio PCA sobre el juego de datos. A pesar de no estar explicado en el material didáctico, se valorará si en lugar de PCA investigáis por vuestra cuenta y aplicáis SVD (Single Value Decomposition).

******
# Práctica 1 - Leonardo Segovia vilchez
******

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Paquetes y librerías..
#install.packages('dendextend')
#install.packages("tidyr")
#install_github("vqv/ggbiplot")
library(tidyr)
library(tibble)
library(dplyr)
library(ggplot2)
library(arules)
library(purrr)
library(dendextend)
library(scales)
library(tidyr)

```

Procedemos a cargar el dataset y echamos un primer vistazo a su estructura. Comprobamos que los datos se han cargado correctamente y que todas las columnans tiene el nombre correcto asignado. 
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Cargamos el juego de datos
travelRawData<- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00484/tripadvisor_review.csv", header=T, sep=",")

# Nombres de los atributos
names(travelRawData) <- c("userId", "artGalleries", "danceClubs", "juiceBars", "restaurants", "museums", "resorts", "parksAndpicnic", "beaches", "theaters", "religiousInstitutions")

# Eliminamos la columna del usuario ya que equivale al index de la tabla.
travelData <- travelRawData[,2:11]

# Añadimos la base de datos a la "search path" de R.
attach(travelData)

# Mostramos las cabecera para comprobar el dataset.
head(travelData, 5)

# Verificamos la estructura del dataset.
str(travelData)
```

Información del dataset seleccionado:

This data set is populated by crawling TripAdvisor.com. Reviews on destinations in 10 categories mentioned across East Asia are considered. 
Each traveler rating is mapped as Excellent (4), Very Good (3), Average (2), Poor (1), and Terrible (0) and average rating is used against each category per user.

Attribute Information:

Attribute 1 : Unique user id

Attribute 2 : Average user feedback on art galleries
Attribute 3 : Average user feedback on dance clubs
Attribute 4 : Average user feedback on juice bars
Attribute 5 : Average user feedback on restaurants
Attribute 6 : Average user feedback on museums
Attribute 7 : Average user feedback on resorts
Attribute 8 : Average user feedback on parks/picnic spots
Attribute 9 : Average user feedback on beaches
Attribute 10 : Average user feedback on theaters
Attribute 11 : Average user feedback on religious institutions

## Justificación de la elección del dataset:
El ambito de los datos no es complejo ya que son valoraciones medias de viajeros por lo que no requiere un conocimiento especifico para entender los datos y poder sacar la información que buscamos. 
El conjuto de datos contiene valores numérico pero se pueden facilmente discretizar por lo que pueden ser usado tanto como supervisados y no supervisados. Además, a parte de las 4 categorías iniciales pretendo también analizar el dataset creando 5 categorias nuevas y intentar buscar algún tipo de associación para mejorar posibles recomendaciones como por ejemplo, si un viajero valora positivamente las galerías de arte y teatros también les gustaría las ir a la playas. Esta informacioón sería muy útil para porder recomendar a los viajeros durante su viaje. 

Este dataset tambien es valido para aplicar un arbol de decision. Además, me gustaria analizar como se comporta con el mismo dataset pero aumentado las etiquetas. Los datasets que me gustaría comparar es uno de 5 valores y el mismo con 10. 

Preguntas que pretendo resolver durante el análisis.
- Que categorias debería potenciar para mejorar substancialmente la valoración media? Me debería centrar en alguna en concreto?
- Hay alguna categoría que parezca importante para los viajeros ya sea por votaciones positivas o negativas? 
- Podría agrupar los viajeros en algún tipo de grupo natural? Si los hubiera, Cuantos hay?
- Nos gustaría recomendar posibles categorías a viajeros durantne su viaje? Por ejemplo si el viajero ha dato una valoracion positiva a un institucion religiosa que acaba de ver, qué otra categoria debería recomendarle para que pueda tener una mejor experiencia?  


## Procesos de limpieza y faltas del conjunto de datos:

Conprobamos la integridad de los datos.
```{r message= FALSE, warning=FALSE}

# Confimamos que están todos los valores 
colSums(travelData == " " | travelData == "" | is.na(travelData | travelData == 0))
```
Confirmamos que no hay datos nulos.

Hacemos un primer análisis.
```{r echo=TRUE, message=FALSE, warning=FALSE}

#Estadísticas básicas
summary(travelData)

```
Vemos que la media con mejor valoración es para el atributo "parksAndpicnic" que resulta ser también el que tiene el valor mínimo más alto de todos los atributos. Por otra parte el atributo resort contiene la valoración máxima maś alta. Posiblemente menos gente opta por hospedarse en resorts de 5 y 4 estrellas por eso la media no es tan alta pero la gente que si lo hace valoran muy positivamente. Podría haber una diferencia sustancial entre las calidades de los distintos "resorts"? 

En el lado contrario las peores valoraciones medias corresponden restaurantes. Luego, "danceClus" es el atributo peor valorado ya que dos clientes del conjunto del dataset lo valorarón con 0. Lo que resulta ser muy poco indicativo teniendo en cuenta las 980 muestras (véase la comprobación en la figura de abajo).  
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Comprobacion del número de observaciones con valoracion igual a 0 para danceClubs.
travelData %>% 
  filter(danceClubs == 0)

# Número total de obserrvaciones.
travelData %>%
  summarise(totalRows = n())

```

Preparamos los datos para poder mostra las medias de forma visual. Por lo que tenemos que prepar el data set con el formato correcto para mostrar la media por categorias.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Preparamos la tabla con el formato correcto para mostrar la media por categorias.
travleDataMean <- travelData %>% 
                  summarise(artGalleries = mean(artGalleries), 
                            danceClubs = mean(danceClubs),
                            juiceBars = mean(juiceBars),
                            restaurants = mean(restaurants),
                            museums = mean(museums),
                            resorts = mean(resorts),
                            parks = mean(parksAndpicnic),
                            beaches = mean(beaches),
                            theaters = mean(theaters),
                            religious = mean(religiousInstitutions))
 
# Pivotamos las columna para poder mostrarlo gráficamente.
travleDataMean <- travleDataMean %>%
  pivot_longer(cols = everything(), names_to = "categories", values_to = "mean") %>%
  arrange(desc(mean))

# Comprobamos el dataset.
head(travleDataMean, 10)
```

Mostramos la medias por categoría de forma ordenada en relacion a su media.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Mostramos la medias por categoría.
ggplot(travleDataMean, aes(reorder(categories, -mean) , mean)) +
  geom_col() +
  ylim(0, 4) +
  xlab("Categories")

```
La gráfica muestra claramente las categorías más altas para "park", "beaches" y "religious" y las peor valoradas "museums", "artGaleries" y "restaurantes"

#### Principal Component Analysis Vs Single Value Decomposition
Intentamos encontrar un subconjunto de los atributos originales que permita obtener modelos de la misma calidad pero reduciendo su dimensionalidad e intentando descubrir que modelo entre "Principal Component Analysis" y "Single Value Decompositionnos" nos da mejor resultado.

REalizamos un análisis conjunto comparando PCA y SVD paso a paso.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Aplicamos Principal components analysis.
travelData.pca <- princomp(travelData, cor = TRUE, scores = TRUE)

# Aplicamos Single value decomposition.
travelData.svd <- prcomp(travelData, cor = TRUE, scores = TRUE)

# Mostramos el resultado.
plot(travelData.pca)
plot(travelData.svd)

```
Obtenemos gráficamente la varianza de cada componente. Siempre el primer componente será el que tenga mayor varianza y irá decreciendo hasta el último componente que tendra la menor. La clave está en definir donde hacer el corte de los componentes sín perder calidad. 

*PCA vs SVD*
El método SVD nos da aparentemente mejor calida ya que sus dos primeras componentes reflejan mayor varianza que con el método PCA. El resultado también se puede medir teniendo en cuenta la pendiente y el codo que forman los componentes en la gráfica. En este caso tambien SVD nos confimra que da un mejor resultado.

Podemos elegir el numero de componentes basandonos en varianza y varianza acumulada porcentual.

*Método mediante "Proportion of variation explained".*
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Mostramos el resultado.
screeplot(travelData.pca, type = "lines")
screeplot(travelData.svd, type = "lines")
```
Siempre las primeras componentes son las que contienen la máxima variación. Las componentes que generen más pendiente representan más variación. En las últimas comoponentes podemos observar una pendiente mucho menos pronunciada, pero de todas maneras no es completamente plana especialmente para PCA, siendo mejor en el caso de SVD pero no parece que podamos reducir drasticamente la dimensionalidad de este conjunto de datos.

*Método mediante "Comulative variance explained".*
Para poder representarlo la variaza acumulada necesitamos trabajar un poco los datos. Luego se puede representar gráficamente.
En este caso he realizado el corte en 90% para mantener un minimo de calidad.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Calculamos la varianzacion.
pca.var <- travelData.pca$sdev^2
svd.var <- travelData.svd$sdev^2

# Calculamos para proporción.
pc.pvar <- pca.var/ sum(pca.var)
sv.pvar <- svd.var/ sum(svd.var)

# Mostramos el resultado.
# PCA
plot(cumsum(pc.pvar), type = "b")
abline(h = 0.9)
# SVD
plot(cumsum(sv.pvar), type = "b")
abline(h = 0.9)

```
Necesitariamos 7 componentes para mantener un 90% de variabilidady para PCA y solo 6 componentes para pasar del 90%. 

*Interpretacion de los atributos PCA*
Podemos utilizar los dos primeros PCA como representacion del conjunto original (high-dimensional data).

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Motramos los pesos o loadings (PCA).
travelData.pca$loadings

```
Observamos también los pesos o loading que han sido omitidos (valores cercanos a cero) de cada atributo. 

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Nos centramos en las dos primeros componentes.
# PCA.
biplot(travelData.pca, col = c("gray","steelblue"), cex = c(0.2, 1.3))
# SVD.
biplot(travelData.svd, col = c("gray","steelblue"), cex = c(0.2, 0.7))

```
La gráfica de arriba representa los "loadings" en azul y "scores" en gris de los dos primeros componentes. El angulo entre los distintos vectores que representan los atributos indican la correlación entre ellos y la longitud del vector es proporcional a la desviación estandar del atributo. De forma analoga podemos ver la correlación entre el atributo y un componentes, mediante el angulo entre el eje del PC y el vector del atributo.

En el método PCA, el componente 1 contiene 3 loading positivas "beach", "religiousInstitution" and "artGalleries" y en el lado opuesto (negativas) el resto de componentes. Ningun vector destaca excesivametne por su longuitud. En cambio en la gráfica usando SVD, se puede ver que la magnitud de los vectores es más desigual, destacando el de "juiceBar." 

Representamos como quedan los "scores" aunque para este conjunto de datos en concreto no podemos sacar mucha más información al respecto.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Guardamos los scores
scoresPca <- data.frame(travelData.pca$scores)

# Mostramos los scores de PC1 y PC2.
# PCA.
ggplot(scoresPca, aes(Comp.1, Comp.2, label = rownames(scoresPca))) +
  geom_text(size = 1, col = "steelblue")

```


#### Análisis del conjunto de datos despues de la discretización:
En el subconjunto inicial hay 5 categorías. Me gustaría aumentar este numero a 10. Reescalamos los valores en el rango para poder discretizar más tarder los valores.

Crearemos las siguientes etiquetas:
10) valores de 10 a 11 -> VeryExcellent 
9)  valores de 9  a 10 -> Excellent 
8)  valores de 8  a  9 -> VeryGood
7)  valores de 7  a  8 -> Good
6)  valores de 6  a  7 -> AboveAverage
5)  valores de 5  a  6 -> Average
4)  valores de 4  a  5 -> Poor
3)  valores de 3  a  4 -> VeryPoor
2)  valores de 2  a  3 -> Terrible 
1)  valores de 1  a  2 -> VeryTerrible 


```{r echo=TRUE, message=FALSE, warning=FALSE}
# Copiamos el dataframe.
traveScaledlData <- travelData

# Reescalado de las columnas
traveScaledlData$artGalleries <- rescale(travelData$artGalleries, to=c(1,11), from = range(0:4))
traveScaledlData$danceClubs <- rescale(travelData$danceClubs, to=c(1,11), from = range(0:4))
traveScaledlData$juiceBars <- rescale(travelData$juiceBars, to=c(1,11), from = range(0:4))
traveScaledlData$restaurants <- rescale(travelData$restaurants, to=c(1,11), from = range(0:4))
traveScaledlData$museums <- rescale(travelData$museums, to=c(1,11), from = range(0:4))
traveScaledlData$resorts <- rescale(travelData$resorts, to=c(1,11), from = range(0:4))
traveScaledlData$parksAndpicnic <- rescale(travelData$parksAndpicnic, to=c(0,10), from = range(0:4))
traveScaledlData$beaches <- rescale(travelData$beaches, to=c(1,11), from = range(0:4))
traveScaledlData$theaters <- rescale(travelData$theaters, to=c(1,11), from = range(0:4))
traveScaledlData$religiousInstitutions <- rescale(travelData$religiousInstitutions, to=c(1,11), from = range(0:4))

# Factorizamos el dataframe.
travelFactor <- traveScaledlData

# Convertimos los datos a numeros enteros.
travelFactor$artGalleries <- as.integer(travelFactor$artGalleries) 
travelFactor$danceClubs <- as.integer(travelFactor$danceClubs) 
travelFactor$juiceBars <- as.integer(travelFactor$juiceBars) 
travelFactor$restaurants <- as.integer(travelFactor$restaurants) 
travelFactor$museums <- as.integer(travelFactor$museums) 
travelFactor$resorts <- as.integer(travelFactor$resorts) 
travelFactor$parksAndpicnic <- as.integer(travelFactor$parksAndpicnic) 
travelFactor$beaches <- as.integer(travelFactor$beaches)
travelFactor$theaters <- as.integer(travelFactor$theaters) 
travelFactor$religiousInstitutions <- as.integer(travelFactor$religiousInstitutions)


# Comprobamos el dataset.
#glimpse(travelFactor)

# Asignamos etiquetas a todos los valores de los atributos.
travelFactor <- travelFactor %>%
  mutate(artGalleries = dplyr::recode(artGalleries, `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) %>%
  
  mutate(danceClubs =dplyr::recode(danceClubs,     `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) %>%
  
  mutate(juiceBars = dplyr::recode(juiceBars,       `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) %>%
    
  mutate(restaurants = dplyr::recode(restaurants,   `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) %>%
  
  mutate(museums = dplyr::recode(museums,           `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) %>%
    
  mutate(resorts = dplyr::recode(resorts,           `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) %>%
    
  mutate(parksAndpicnic = dplyr::recode(parksAndpicnic, `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) %>%
    
  mutate(beaches = dplyr::recode(beaches, `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) %>%
    
  mutate(theaters = dplyr::recode(theaters, `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) %>%
    
  mutate(religiousInstitutions = dplyr::recode(religiousInstitutions, `1` = "0veryTerrible", `2` = "1terrible",`3` = "2veryPoor",
                                            `4` = "3poor", `5` = "4average",`6` = "5aboveAverage", `7` = "6good", 
                                            `8` = "7veryGood",`9` = "8excellent",`10` = "9veryExcellent",)) 


# Discretizamos todas las variables.
cols<-c("artGalleries", "danceClubs", "juiceBars", "restaurants", "museums", "resorts", "parksAndpicnic", "beaches", "theaters", "religiousInstitutions")
for (i in cols){
  travelFactor[,i] <- as.factor(travelFactor[,i])
}

# Comprobamos el dataset.
glimpse(travelFactor)

```

Trabajamos el data set para poder visualizar las categorías en función del feedback de los viajeros. Para poder conseguir nuestro objetivo tranformamos el dataset a dos columnas (dataframe tipo "long"), una con el atributo "categorias" y el otro con "feedback" que contiene las valoraciones de los viajeros.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Pivotamos las columna para poder mostrarlo gráficamente.
travelFactorLong <- travelFactor %>%
  pivot_longer(cols = everything(), names_to = "categories", values_to = "feedback") %>%
  arrange(desc(feedback))

# Comprobamos el dataset mostrando las tres primeras columnas.
head(travelFactorLong,3)

```

Mostramos las frecuencias por categoria
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Tabla de frecuencias
table(travelFactorLong)

```
De esta tabla se pueden destacar varios aspectos. Destaca que el extremo negativo "veryTerrible" tiene más frecuencias que el extremo positivo "veryExcellent" y otro aspecto esperado es que las valoraciones medias  como pueden ser "good" o "average" tiene un mayor numero de frecuencias.

En la tabla de abajo observamos que el máximo porcetaje es de 20% sobre la valoracion "good" sobre el total de las categorías .
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Tabla de frecuencias
prop.table(table(travelFactorLong$feedback))*100

```

La siguiente gráfica muestra el peso de todas las categorías en funcion de la valoración. Se puede hacer difícil obtener informacion de la gráfica ya que muestra toda la suma de las valoraciones y las columnas quedan muy divididas. 
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Mostramoslas categorías como funcion de feedback
travelFactorLong %>%
  ggplot(aes(feedback, fill=categories)) +
    geom_bar() +
    scale_x_discrete(labels = abbreviate)

```

En está ocasion filtramos solo las categoría que tengan un peso importante poniendo el limite a 200 valoraciones. Esto nos ayuda a tener una mejor idea de como queda la distribucion de las categorias en funcion de las valoraciones. Vemos como "restaurantes" tiene una muy mala valoración. Contiene el mayor numero de valoraciones negativas de todas las demás categorías.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Filtramos para feedback mayores a 200.
travelFactorLong %>%
  group_by(categories, feedback) %>%
  summarise(counter = n()) %>%
  filter(counter > 200) %>%
  # Mostramoslas categorías como funcion de feedback.
ggplot(aes(x = feedback, y = counter, fill= categories)) +
  geom_col() +
  scale_x_discrete(labels = abbreviate)

```

Analizo más en detalla la categoría restaurantes.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Filtramos para la categoría restaurantes.
travelFactorLong %>%
  group_by(categories, feedback) %>%
  summarise(counter = n()) %>%
  filter(counter > 0 & (categories == "restaurants")) %>%
  # Mostramoslas categorías como funcion de feedback.
  ggplot(aes(reorder(x = feedback, counter), y = counter, fill= categories)) +
    geom_col() +
    scale_x_discrete(labels = abbreviate)

```
Comprobamos que hay una opinion generalizada sobre los restaurantes. Podemos trabajar con la hipotesis que la gente valora los restaurantes a la hora de viajar y que podrían esperan una minima calidad de ellos. Sin duda este es un aspecto a mejorar para poder aumentar la valoración en general de los viajeros. Se podría trabajar en otras encuestas sobre que es lo que hace que la valoracion sea tan baja dentro de esta categoría; higiene, materias primas, localizacion, servicio ..etc.

Ahora mostramos las mejores valoraciones.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Filtramos para las categoría mejor valoradas.
travelFactorLong %>%
  group_by(categories, feedback) %>%
  summarise(counter = n()) %>%
  filter(counter > 0 & (categories == "parksAndpicnic" | categories == "beaches" | categories == "religiousInstitutions")) %>%
  # Mostramoslas categorías como funcion de feedback.
  ggplot(aes(reorder(x = feedback, -counter), y = counter)) + 
  geom_col() + facet_wrap(~categories) +
  xlab("Feedback") +
  scale_x_discrete(labels = abbreviate)

```
Por otro lado las mejores valoraciones destacan tres categorias: Playas y parques que están centradas con un buena valorción "good" y luego las valoraciones para instituciones religuioas se mueven entre "aboveAverage", "good", "veryGood" y incluso "excellent" y "veryEcellent"

Veamos como quedaría la categorías de "veryExceptional" y "exceptional" aunque el numero no sea muy indicativo ya que esta por debajo de las 200 valoraciones. 
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Filtramos para la categoría restaurantes.
travelFactorLong %>%
  group_by(categories, feedback) %>%
  summarise(counter = n()) %>%
  filter(counter > 0 & (feedback == "9veryExcellent" | feedback == "8excellent" )) %>%
  # Mostramoslas categorías como funcion de feedback.
  ggplot(aes(reorder(x = feedback, counter), y = counter, fill= categories)) +
    geom_col() +
    xlab("Feedback") +
    scale_x_discrete(labels = abbreviate)

```
Vemos que "religiousInstitution" es la mejor posicionada por el numero de valoraciones positivas.

Centrandonos ahora sobre el atributo restaurantes, veamos como se relaciona con otras variables como; "theaters", "juiceBars", "resorts" y "museums". 
Ya que estos atributos eran atributos relevantes en el análisis SVD.
```{r echo=TRUE, message=FALSE, warning=FALSE}

# Mostra restaurantes en funcion de teatros.
ggplot(travelFactor, aes(theaters, fill=restaurants)) +
    geom_bar() +
    scale_x_discrete(labels = abbreviate)

```
La mayoría que valora los teatro "poor" y "average" también lo hace muy negativamente para restaurates con "terrible".

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Mostra restaurantes en funcion de teatros.
ggplot(travelFactor, aes(juiceBars, fill=restaurants)) +
    geom_bar() +
    scale_x_discrete(labels = abbreviate)

```
Obserbamos que hay valoraciones positivas para "juiceBar" pero sin embargo son muy malas para restaurantes.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Mostra restaurantes en funcion de resorts.
ggplot(travelFactor, aes(resorts, fill=restaurants)) +
    geom_bar() +
    scale_x_discrete(labels = abbreviate)

```
En este caso la distribución esta más centrada sobre sobre "average" con forma de una distribucion normal.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Mostra restaurantes en funcion de museos
ggplot(travelFactor, aes(museums, fill=restaurants)) +
    geom_bar() +
    scale_x_discrete(labels = abbreviate) +
    scale_y_discrete(labels = abbreviate)

```
La mayoría que valora negativametne los museos tambien lo hace para restaurantes.

******
# Rúbrica
******
20%. Justificación razonada y completa de la elección del juego de datos donde se detalla el potencial analítico que se intuye y que se podrán crear los modelos solicitados. El estudiante deberá visitar los siguientes portales -o otros que desee- de datos abiertos para seleccionar su juego de datos:
  + [Datos.gob.es](https://datos.gob.es/es/catalogo?q=&frequency=%7B"type"%3A+"months"%2C+"value"%3A+"1"%7D&sort=score+desc%2C+metadata_modified+desc)
  + [UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets.php)
  + [Datasets Wikipedia](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)
  + [Datos abierto Madrid](https://datos.madrid.es/portal/site/egob/)
  + [Datos abiertos Barcelona](https://opendata-ajuntament.barcelona.cat/ca/)
  + [London Datastore](https://data.london.gov.uk/)
  + [NYC OpenData](https://opendata.cityofnewyork.us/)
* 25%. Redactar de forma clara y completa la información extraída del análisis exploratorio. Distribuciones, correlaciones, anomalías, ... ¿Qué se quiere obtener? Cómo se hace? ¿Qué se obtiene? Como se interpreta?
* 25%. Explicación clara de cualquier tarea de limpieza o acondicionado que se realiza. Justificando el motivo y mencionando las ventajas de la acción tomada.
* 25%. Realizar la discretización de los datos que lo requieran utilizando los diversos métodos vistos y justificando su elección.
*  5%. Se realiza un proceso de PCA o SVD donde se aprecia mediante explicaciones y comentarios que el estudiante entiende todos los pasos y se comenta extensamente el resultado final obtenido.



